# This workflow will install Python dependencies, run tests and lint with a variety of Python versions
# For more information see: https://help.github.com/actions/language-and-framework-guides/using-python-with-github-actions

name: Push Binary Nightly

on:
  # run every day at 22:15
  schedule:
    - cron:  '05 22 * * *'
  # or manually trigger it
  workflow_dispatch:

jobs:

  # build on cpu hosts and upload to GHA
  build_on_cpu:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [linux.2xlarge]
    steps:
    # Checkout the repository to the GitHub Actions runner
    - name: Check ldd --version
      run: ldd --version
    - name: Checkout
      uses: actions/checkout@v2
      with:
        submodules: true
    # Update references
    - name: Git Sumbodule Update
      run: |
        git submodule init
        git submodule update --remote --recursive
        cd third_party/fbgemm/
        git submodule sync
        git submodule update --init --recursive
        git log
    # manually install python3.8 since self-hosted machine does not support the python action feature
    - name: check amazon-linux-extras
      run: |
        sudo yum install -y amazon-linux-extras
        amazon-linux-extras | grep -i python
    - name: update python
      run: |
        sudo amazon-linux-extras install python3.8
    - name: Update pip
      run: |
        sudo yum update -y
        sudo yum -y install git python3-pip
        sudo pip3 install --upgrade pip
    - name: create virtual env
      run: |
        sudo pip3 install --upgrade virtualenv
        virtualenv build_binary_3.8 -p python3.8
    - name: check python version
      run: |
        source build_binary_3.8/bin/activate
        python --version
    - name: Install CUDA 11.3
      shell: bash
      run: |
        sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
        sudo yum-config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/cuda-rhel7.repo
        sudo yum clean expire-cache
        sudo yum install -y nvidia-driver-latest-dkms
        sudo yum install -y cuda-11-3
        sudo yum install -y cuda-drivers
        sudo yum install -y libcudnn8-devel
    - name: setup Path
      run: |
        echo /usr/local/cuda-11.3/bin >> $GITHUB_PATH
        echo /usr/local/bin >> $GITHUB_PATH
    - name: nvcc check
      run: |
        nvcc --version
    - name: Install PyTorch
      shell: bash
      run: |
        source build_binary_3.8/bin/activate
        pip3 install --pre torch -f https://download.pytorch.org/whl/nightly/cu113/torch_nightly.html
    - name: Install Dependencies
      shell: bash
      run: |
        source build_binary_3.8/bin/activate
        pip3 install -r requirements.txt
    - name: Test Installation of dependencies
      run: |
        source build_binary_3.8/bin/activate
        python3 -c "import torch.distributed"
        echo "torch.distributed succeeded"
        python3 -c "import skbuild"
        echo "skbuild succeeded"
        python3 -c "import numpy"
        echo "numpy succeeded"
    - name: Build TorchRec Nightly
      run: |
        source build_binary_3.8/bin/activate
        rm -r dist || true
        python setup.py bdist_wheel --override_name torchrec-nightly --TORCH_CUDA_ARCH_LIST '7.0;8.0'
    - name: Upload wheel as GHA artifact
      uses: actions/upload-artifact@v2
      with:
        name: torchrec_nightly.whl
        path: dist/*.whl

  # download from GHA, test on gpu and push to pypi
  test_on_gpu:
    needs: build_on_cpu
    runs-on: linux.4xlarge.nvidia.gpu
    steps:
    - name: Check ldd --version
      run: ldd --version
    - name: check cpu info
      shell: bash
      run: |
        cat /proc/cpuinfo
    - name: check distribution info
      shell: bash
      run: |
        cat /proc/version
    - name: Display EC2 information
      shell: bash
      run: |
        set -euo pipefail
        function get_ec2_metadata() {
          # Pulled from instance metadata endpoint for EC2
          # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
          category=$1
          curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
        }
        echo "ami-id: $(get_ec2_metadata ami-id)"
        echo "instance-id: $(get_ec2_metadata instance-id)"
        echo "instance-type: $(get_ec2_metadata instance-type)"
    - name: check gpu info
      shell: bash
      run: |
        sudo yum install lshw -y
        sudo lshw -C display
    # Checkout the repository to the GitHub Actions runner
    - name: Checkout
      uses: actions/checkout@v2
      with:
        submodules: true
    # Update references
    - name: Git Sumbodule Update
      run: |
        git submodule init
        git submodule update --remote --recursive
        cd third_party/fbgemm/
        git submodule sync
        git submodule update --init --recursive
        git log
    # manually install python3.8 since self-hosted machine does not support the python action feature
    - name: check amazon-linux-extras
      run: |
        sudo yum install -y amazon-linux-extras
        amazon-linux-extras | grep -i python
    - name: update python
      run: |
        sudo amazon-linux-extras install python3.8
    - name: Update pip
      run: |
        sudo yum update -y
        sudo yum -y install git python3-pip
        sudo pip3 install --upgrade pip
    - name: create virtual env
      run: |
        sudo pip3 install --upgrade virtualenv
        virtualenv build_binary_3.8 -p python3.8
    - name: check python version
      run: |
        source build_binary_3.8/bin/activate
        python --version
    - name: Install CUDA 11.3
      shell: bash
      run: |
        sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
        sudo yum-config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/cuda-rhel7.repo
        sudo yum clean expire-cache
        sudo yum install -y nvidia-driver-latest-dkms
        sudo yum install -y cuda-11-3
        sudo yum install -y cuda-drivers
        sudo yum install -y libcudnn8-devel
    - name: setup Path
      run: |
        echo /usr/local/cuda-11.3/bin >> $GITHUB_PATH
        echo /usr/local/bin >> $GITHUB_PATH
    - name: nvcc check
      run: |
        nvcc --version
    - name: Install PyTorch
      shell: bash
      run: |
        source build_binary_3.8/bin/activate
        pip3 install --pre torch -f https://download.pytorch.org/whl/nightly/cu113/torch_nightly.html
    - name: Install Dependencies
      shell: bash
      run: |
        source build_binary_3.8/bin/activate
        pip3 install -r requirements.txt
    - name: Test Installation of dependencies
      run: |
        source build_binary_3.8/bin/activate
        python3 -c "import torch.distributed"
        echo "torch.distributed succeeded"
        python3 -c "import skbuild"
        echo "skbuild succeeded"
        python3 -c "import numpy"
        echo "numpy succeeded"
    # download wheel from GHA
    - name: Download wheel
      uses: actions/download-artifact@v2
      with:
        name: torchrec_nightly.whl
    - name: Display structure of downloaded files
      run: ls -R
    - name: Install TorchRec Nightly
      run: |
        source build_binary_3.8/bin/activate
        rm -r dist || true
        pip3 install *.whl
    - name: Test fbgemm_gpu and torchrec installation
      shell: bash
      run: |
        source build_binary_3.8/bin/activate
        python3 -c "import fbgemm_gpu"
        python3 -c "import torchrec"
    - name: Test with pytest
      # remove this line when we fixed all the unit tests
      continue-on-error: true
      run: |
        source build_binary_3.8/bin/activate
        pip3 install pytest
        cd torchrec
        python3 -m pytest -s -v -W ignore::pytest.PytestCollectionWarning --continue-on-collection-errors
    # Push to Pypi
    - name: Push TorchRec Binary to Test PYPI
      env:
          PYPI_TOKEN: ${{ secrets.PYPI_TOKEN }}
      run: |
        source build_binary_3.8/bin/activate
        pip3 install twine
        twine upload \
            --username __token__ \
            --password "$PYPI_TOKEN" \
            *.whl
