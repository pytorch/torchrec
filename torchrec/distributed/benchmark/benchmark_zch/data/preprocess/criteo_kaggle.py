#!/usr/bin/env python3
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

import argparse
import os
from typing import Any, Dict, List

from torch import distributed as dist
from torch.utils.data import DataLoader
from torchrec.datasets.criteo import CAT_FEATURE_COUNT, InMemoryBinaryCriteoIterDataPipe

STAGES = ["train", "val", "test"]


def get_criteo_kaggle_dataloader(
    args: argparse.Namespace,
    configs: Dict[str, Any],
    stage: str = "train",  # "train", "val", "test"
) -> DataLoader:
    dir_path = configs["dataset_path"]
    sparse_part = "sparse.npy"
    datapipe = InMemoryBinaryCriteoIterDataPipe

    # criteo_kaggle has no validation set, so use 2nd half of training set for now.
    # Setting stage to "test" will get the 2nd half of the dataset.
    # Setting root_name to "train" reads from the training set file.
    (root_name, stage) = ("train", "train") if stage == "train" else ("train", "test")
    stage_files: List[List[str]] = [
        [os.path.join(dir_path, f"{root_name}_dense.npy")],
        [os.path.join(dir_path, f"{root_name}_{sparse_part}")],
        [os.path.join(dir_path, f"{root_name}_labels.npy")],
    ]
    batch_size = configs["batch_size"] if args.batch_size is None else args.batch_size
    dataloader = DataLoader(
        datapipe(
            stage,
            *stage_files,  # pyre-ignore[6]
            batch_size=batch_size,
            rank=dist.get_rank(),
            world_size=dist.get_world_size(),
            drop_last=False,
            shuffle_batches=args.shuffle_batches,
            hashes=(
                [args.num_embeddings] * CAT_FEATURE_COUNT
                if args.input_hash_size is None
                else ([args.input_hash_size] * CAT_FEATURE_COUNT)
            ),
        ),
        batch_size=None,
        collate_fn=lambda x: x,
    )
    return dataloader
