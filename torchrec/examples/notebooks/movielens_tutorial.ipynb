{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchrec MovieLens Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. Instantiating MovieLens-25M dataset\n",
    "2. Defining model\n",
    "3. Training and evaluating model\n",
    "4. Finding similar movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instantiating MovieLens-25M dataset\n",
    "\n",
    "To start, we can load the MovieLens-25M dataset using `torchrec.datasets.movielens.movielens_25m`. The function loads just the user-movie ratings data in `ratings.csv` by default; we call the function with `include_movies_data=True` such that it adds movie data from `movies.csv` to each user-movie sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrec.datasets.movielens import movielens_25m\n",
    "\n",
    "dp = movielens_25m(\"/home/jeffhwang/local/datasets/ml-25m\", include_movies_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out a single sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'userId': 1,\n",
       " 'movieId': 296,\n",
       " 'rating': 5.0,\n",
       " 'timestamp': 1147880044,\n",
       " 'title': 'Pulp Fiction (1994)',\n",
       " 'genres': 'Comedy|Crime|Drama|Thriller'}"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "bento_obj_id": "139906334494848"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems reasonable.\n",
    "\n",
    "Next, we instantiate datapipes representing training and validation data splits and apply shuffling and batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrec.datasets.utils import rand_split_train_val\n",
    "\n",
    "train_dp, val_dp = rand_split_train_val(dp, 0.9)\n",
    "batched_train_dp = train_dp.shuffle(buffer_size=int(1e5)).batch(8192)\n",
    "batched_val_dp = val_dp.batch(8192)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out that the integer user ids and movie ids referenced by the dataset aren't contiguous. Let's remap them to contiguous values so that we can use them with `torch.nn.Embedding` more easily downstream.\n",
    "\n",
    "To do so, we first populate dictionaries that map movie and user ids to ids in contiguous ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "contig_movie_ids = {}\n",
    "contig_user_ids = {}\n",
    "movie_id_to_title_genre = {}\n",
    "\n",
    "available_movie_id = 0\n",
    "available_user_id = 0\n",
    "for sample in dp:\n",
    "    if sample[\"movieId\"] not in contig_movie_ids:\n",
    "        contig_movie_ids[sample[\"movieId\"]] = available_movie_id\n",
    "        available_movie_id += 1\n",
    "    if sample[\"userId\"] not in contig_user_ids:\n",
    "        contig_user_ids[sample[\"userId\"]] = available_user_id\n",
    "        available_user_id += 1\n",
    "    movie_id_to_title_genre[sample[\"movieId\"]] = (sample[\"title\"], sample[\"genres\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", and then define a function `_transform` that uses those dictionaries to remap movie and user ids for a batch of data. While we're at it, we'll also have `_transform` reformat the batch as tensors representing user ids, movie ids, and labels (numerical movie ratings given by users)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def _transform(batch):\n",
    "    user_ids = torch.tensor([contig_user_ids[sample[\"userId\"]] for sample in batch], dtype=torch.int32)\n",
    "    movie_ids = torch.tensor([contig_movie_ids[sample[\"movieId\"]] for sample in batch], dtype=torch.int32)\n",
    "    labels = torch.tensor([sample[\"rating\"] for sample in batch], dtype=torch.float)\n",
    "    return user_ids, movie_ids, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we configure our training and validation datapipes to apply `_transform` to each batch of data using `map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_train_dp = batched_train_dp.map(_transform)\n",
    "preproc_val_dp = batched_val_dp.map(_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, `preproc_train_dp` and `preproc_val_dp` are set up to produce the data that our model expects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining model\n",
    "\n",
    "Next, we define the model we're going to train. We'll go with a simplified two-tower model `TwoTowerModel` resembling a matrix factorization model that attempts to learn a low-rank approximation of the user-movie ratings matrix. More specifically, we want to find matrices $U \\in \\mathbb{R}^{u \\times d}$ and $M \\in \\mathbb{R}^{m \\times d}$ such that $U M^T \\approx A$, where each row in $U$ represents a user embedding of dimension $d$ and each row in $M$ a movie embedding also of dimension $d$. Once we find matrices $U$ and $M$, we can infer the rating that the $i$-th user gives the $j$-th movie as $u_i^T \\cdot m_j^T$, i.e. the dot product of the $i$-th row in $U$ and $j$-th row in $M$.\n",
    "\n",
    "`TwoTowerModel` represents $U$ and $M$ as embedding tables — instances of `torch.nn.Embedding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoTowerModel(torch.nn.Module):\n",
    "    def __init__(self, num_embeddings_0, num_embeddings_1, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.model_0 = torch.nn.Embedding(num_embeddings_0, embedding_dim)\n",
    "        self.model_1 = torch.nn.Embedding(num_embeddings_1, embedding_dim)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        embeddings_0 = self.model_0(input[0])\n",
    "        embeddings_1 = self.model_1(input[1])\n",
    "        return torch.sum(embeddings_0 * embeddings_1, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training and evaluating model\n",
    "We're ready to train our model. Let's instantiate the model we just defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoTowerModel(\n",
    "    len(contig_user_ids),\n",
    "    len(contig_movie_ids),\n",
    "    32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", instantiate our loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", and define our train and test loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dp, model, loss_fn, optimizer):\n",
    "    for batch, (users, movies, labels) in enumerate(dp):\n",
    "        pred = model((users, movies))\n",
    "        loss = loss_fn(pred, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(labels)\n",
    "            print(f\"loss: {loss:>7f}; batch: {batch}\")\n",
    "\n",
    "def test_loop(dp, model, loss_fn):\n",
    "    test_loss = 0\n",
    "    batch_count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (users, movies, labels) in enumerate(dp):\n",
    "            pred = model((users, movies))\n",
    "            test_loss += loss_fn(pred, labels).item()\n",
    "            batch_count += 1\n",
    "    \n",
    "    print(f\"Test loss: {test_loss / batch_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 46.037308; batch: 0\n",
      "loss: 38.465691; batch: 100\n",
      "loss: 36.324226; batch: 200\n",
      "loss: 32.601963; batch: 300\n",
      "loss: 31.454372; batch: 400\n",
      "loss: 28.976149; batch: 500\n",
      "loss: 28.139290; batch: 600\n",
      "loss: 26.033499; batch: 700\n",
      "loss: 24.564535; batch: 800\n",
      "loss: 24.771233; batch: 900\n",
      "loss: 23.982235; batch: 1000\n",
      "loss: 23.603422; batch: 1100\n",
      "loss: 22.836342; batch: 1200\n",
      "loss: 22.366283; batch: 1300\n",
      "loss: 21.981915; batch: 1400\n",
      "loss: 20.872860; batch: 1500\n",
      "loss: 21.898285; batch: 1600\n",
      "loss: 21.096714; batch: 1700\n",
      "loss: 21.146002; batch: 1800\n",
      "loss: 20.964083; batch: 1900\n",
      "loss: 20.462320; batch: 2000\n",
      "loss: 19.815897; batch: 2100\n",
      "loss: 20.486073; batch: 2200\n",
      "loss: 19.710440; batch: 2300\n",
      "loss: 19.632444; batch: 2400\n",
      "loss: 20.404284; batch: 2500\n",
      "loss: 19.290865; batch: 2600\n",
      "loss: 19.100807; batch: 2700\n",
      "Test loss: 18.480480686511868\n",
      "loss: 18.086700; batch: 0\n",
      "loss: 17.338081; batch: 100\n",
      "loss: 17.585329; batch: 200\n",
      "loss: 17.180315; batch: 300\n",
      "loss: 17.549704; batch: 400\n",
      "loss: 17.645161; batch: 500\n",
      "loss: 17.722076; batch: 600\n",
      "loss: 17.680521; batch: 700\n",
      "loss: 17.408783; batch: 800\n",
      "loss: 17.525982; batch: 900\n",
      "loss: 17.376921; batch: 1000\n",
      "loss: 17.426014; batch: 1100\n",
      "loss: 17.220980; batch: 1200\n",
      "loss: 16.952011; batch: 1300\n",
      "loss: 17.134249; batch: 1400\n",
      "loss: 17.048244; batch: 1500\n",
      "loss: 17.227037; batch: 1600\n",
      "loss: 17.148981; batch: 1700\n",
      "loss: 17.023678; batch: 1800\n",
      "loss: 16.871969; batch: 1900\n",
      "loss: 17.318562; batch: 2000\n",
      "loss: 16.544071; batch: 2100\n",
      "loss: 16.600208; batch: 2200\n",
      "loss: 16.858044; batch: 2300\n",
      "loss: 16.681547; batch: 2400\n",
      "loss: 17.338814; batch: 2500\n",
      "loss: 16.720623; batch: 2600\n",
      "loss: 16.843494; batch: 2700\n",
      "Test loss: 16.428918199601515\n",
      "loss: 16.221512; batch: 0\n",
      "loss: 16.358212; batch: 100\n",
      "loss: 15.603410; batch: 200\n",
      "loss: 15.793720; batch: 300\n",
      "loss: 16.120581; batch: 400\n",
      "loss: 15.609840; batch: 500\n",
      "loss: 15.646942; batch: 600\n",
      "loss: 15.803013; batch: 700\n",
      "loss: 15.460732; batch: 800\n",
      "loss: 15.707933; batch: 900\n",
      "loss: 15.387153; batch: 1000\n",
      "loss: 15.972283; batch: 1100\n",
      "loss: 15.815547; batch: 1200\n",
      "loss: 15.986384; batch: 1300\n",
      "loss: 15.839499; batch: 1400\n",
      "loss: 16.124321; batch: 1500\n",
      "loss: 15.958432; batch: 1600\n",
      "loss: 15.983364; batch: 1700\n",
      "loss: 15.935410; batch: 1800\n",
      "loss: 15.357247; batch: 1900\n",
      "loss: 15.969000; batch: 2000\n",
      "loss: 15.663831; batch: 2100\n",
      "loss: 15.580512; batch: 2200\n",
      "loss: 15.733629; batch: 2300\n",
      "loss: 15.704931; batch: 2400\n",
      "loss: 15.884350; batch: 2500\n",
      "loss: 15.432696; batch: 2600\n",
      "loss: 15.874282; batch: 2700\n",
      "Test loss: 15.481964759577334\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "for __ in range(epochs):\n",
    "    train_loop(preproc_train_dp, model, loss_fn, optimizer)\n",
    "    test_loop(preproc_val_dp, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got a trained model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Finding similar movies\n",
    "For kicks, let's see if we can use our model's trained embeddings to find movies that are most similar to some query movie. In theory, movies with embeddings that are similar should themselves be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "contig_to_movie_id = {v: k for k, v in contig_movie_ids.items()}\n",
    "\n",
    "def get_topk_sim_movies(movie_id, k=20):\n",
    "    embedding = model.model_1(torch.tensor([contig_movie_ids[movie_id]]))\n",
    "    movie_embeddings = model.get_parameter(\"model_1.weight\")\n",
    "    movie_similarities = torch.sum(embedding * movie_embeddings, axis=1) / torch.maximum(torch.norm(embedding) * torch.norm(movie_embeddings, dim=1), torch.ones(movie_embeddings.shape[0]) * 1e-12)\n",
    "    topk_sim = torch.topk(movie_similarities, 20)\n",
    "    contig_ids = topk_sim.indices.tolist()\n",
    "    return [\n",
    "        (*movie_id_to_title_genre[contig_to_movie_id[movie_id]], contig_to_movie_id[movie_id]) \n",
    "        for movie_id in contig_ids\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Drive (2011)', 'Crime|Drama|Film-Noir|Thriller', 88129),\n",
       " ('Wolf of Wall Street, The (2013)', 'Comedy|Crime|Drama', 106782),\n",
       " ('Ex Machina (2015)', 'Drama|Sci-Fi|Thriller', 115713),\n",
       " ('Shutter Island (2010)', 'Drama|Mystery|Thriller', 74458),\n",
       " ('Girl with the Dragon Tattoo, The (2011)', 'Drama|Thriller', 91658),\n",
       " ('True Grit (2010)', 'Western', 82459),\n",
       " ('Gone Girl (2014)', 'Drama|Thriller', 112556),\n",
       " ('Looper (2012)', 'Action|Crime|Sci-Fi', 96610),\n",
       " ('127 Hours (2010)', 'Adventure|Drama|Thriller', 81562),\n",
       " ('No Country for Old Men (2007)', 'Crime|Drama', 55820),\n",
       " ('Black Swan (2010)', 'Drama|Thriller', 81591),\n",
       " ('Big Short, The (2015)', 'Drama', 148626),\n",
       " ('Grand Budapest Hotel, The (2014)', 'Comedy|Drama', 109374),\n",
       " ('Skyfall (2012)', 'Action|Adventure|Thriller|IMAX', 96079),\n",
       " ('Dark Knight Rises, The (2012)', 'Action|Adventure|Crime|IMAX', 91529),\n",
       " ('Sherlock Holmes: A Game of Shadows (2011)',\n",
       "  'Action|Adventure|Comedy|Crime|Mystery|Thriller',\n",
       "  91542),\n",
       " ('Gran Torino (2008)', 'Crime|Drama', 64614),\n",
       " ('Whiplash (2014)', 'Drama', 112552),\n",
       " ('American History X (1998)', 'Crime|Drama', 2329),\n",
       " ('Edge of Tomorrow (2014)', 'Action|Sci-Fi|IMAX', 111759)]"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "bento_obj_id": "139905706007040"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drive\n",
    "get_topk_sim_movies(88129)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Lost in Translation (2003)', 'Comedy|Drama|Romance', 6711),\n",
       " ('Blade (1998)', 'Action|Horror|Thriller', 2167),\n",
       " ('O Brother, Where Art Thou? (2000)', 'Adventure|Comedy|Crime', 4027),\n",
       " ('Minority Report (2002)', 'Action|Crime|Mystery|Sci-Fi|Thriller', 5445),\n",
       " ('Bowling for Columbine (2002)', 'Documentary', 5669),\n",
       " ('Lord of War (2005)', 'Action|Crime|Drama|Thriller|War', 36529),\n",
       " ('Dark City (1998)', 'Adventure|Film-Noir|Sci-Fi|Thriller', 1748),\n",
       " ('Rocky (1976)', 'Drama', 1954),\n",
       " ('Best in Show (2000)', 'Comedy', 3911),\n",
       " ('Deliverance (1972)', 'Adventure|Drama|Thriller', 2871),\n",
       " ('Eternal Sunshine of the Spotless Mind (2004)',\n",
       "  'Drama|Romance|Sci-Fi',\n",
       "  7361),\n",
       " ('Crouching Tiger, Hidden Dragon (Wo hu cang long) (2000)',\n",
       "  'Action|Drama|Romance',\n",
       "  3996),\n",
       " ('Fahrenheit 9/11 (2004)', 'Documentary', 8622),\n",
       " ('Back to the Future Part III (1990)',\n",
       "  'Adventure|Comedy|Sci-Fi|Western',\n",
       "  2012),\n",
       " ('Boogie Nights (1997)', 'Drama', 1673),\n",
       " ('Run Lola Run (Lola rennt) (1998)', 'Action|Crime', 2692),\n",
       " ('Con Air (1997)', 'Action|Adventure|Thriller', 1552),\n",
       " (\"Ocean's Eleven (2001)\", 'Crime|Thriller', 4963),\n",
       " ('Austin Powers: International Man of Mystery (1997)',\n",
       "  'Action|Adventure|Comedy',\n",
       "  1517),\n",
       " ('Bad Boys (1995)', 'Action|Comedy|Crime|Drama|Thriller', 145)]"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "bento_obj_id": "139906355972864"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lost in Translation\n",
    "get_topk_sim_movies(6711)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ratatouille (2007)', 'Animation|Children|Drama', 50872),\n",
       " ('Harry Potter and the Goblet of Fire (2005)',\n",
       "  'Adventure|Fantasy|Thriller|IMAX',\n",
       "  40815),\n",
       " ('Toy Story 2 (1999)', 'Adventure|Animation|Children|Comedy|Fantasy', 3114),\n",
       " ('Ice Age (2002)', 'Adventure|Animation|Children|Comedy', 5218),\n",
       " ('Monsters, Inc. (2001)',\n",
       "  'Adventure|Animation|Children|Comedy|Fantasy',\n",
       "  4886),\n",
       " ('Harry Potter and the Chamber of Secrets (2002)', 'Adventure|Fantasy', 5816),\n",
       " ('Wedding Singer, The (1998)', 'Comedy|Romance', 1777),\n",
       " ('Armageddon (1998)', 'Action|Romance|Sci-Fi|Thriller', 1917),\n",
       " ('WALL·E (2008)', 'Adventure|Animation|Children|Romance|Sci-Fi', 60069),\n",
       " ('Finding Nemo (2003)', 'Adventure|Animation|Children|Comedy', 6377),\n",
       " ('Indiana Jones and the Temple of Doom (1984)',\n",
       "  'Action|Adventure|Fantasy',\n",
       "  2115),\n",
       " ('Signs (2002)', 'Horror|Sci-Fi|Thriller', 5502),\n",
       " ('Toy Story 3 (2010)',\n",
       "  'Adventure|Animation|Children|Comedy|Fantasy|IMAX',\n",
       "  78499),\n",
       " ('My Big Fat Greek Wedding (2002)', 'Comedy|Romance', 5299),\n",
       " ('Harry Potter and the Prisoner of Azkaban (2004)',\n",
       "  'Adventure|Fantasy|IMAX',\n",
       "  8368),\n",
       " ('Star Wars: Episode VI - Return of the Jedi (1983)',\n",
       "  'Action|Adventure|Sci-Fi',\n",
       "  1210),\n",
       " ('Black Hawk Down (2001)', 'Action|Drama|War', 5010),\n",
       " (\"Harry Potter and the Sorcerer's Stone (a.k.a. Harry Potter and the Philosopher's Stone) (2001)\",\n",
       "  'Adventure|Children|Fantasy',\n",
       "  4896),\n",
       " ('Big Hero 6 (2014)', 'Action|Animation|Comedy', 115617),\n",
       " ('Hot Fuzz (2007)', 'Action|Comedy|Crime|Mystery', 51255)]"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "bento_obj_id": "139905661210368"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ratatouille\n",
    "get_topk_sim_movies(50872)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think? Can we do better?"
   ]
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "disseminate_notebook_id": {
   "notebook_id": "552815895724328"
  },
  "disseminate_notebook_info": {
   "bento_version": "20210627-210324",
   "description": "",
   "hide_code": false,
   "hipster_group": "",
   "kernel_build_info": {
    "deps": [
     "//caffe2/caffe2/fb/ifbpy:all_pytorch_and_caffe2_deps",
     "//github/third-party/PyTorchLightning/pytorch-lightning:lib"
    ],
    "external_deps": []
   },
   "no_uii": true,
   "notebook_number": "954867",
   "others_can_edit": false,
   "reviewers": "",
   "revision_id": "489172452146612",
   "tags": "",
   "tasks": "",
   "title": "torchrec movielens"
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "bento_kernel_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
